# DataFlow - Un outil ETL modulaire en Python

## ğŸ“‹ Table des matiÃ¨res
- [Introduction](#-introduction)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Architecture](#-architecture)
- [Structure du projet](#-structure-du-projet)
- [Documentation technique](#-documentation-technique)
- [Contribution](#-contribution)
- [Licence](#-licence)

## ğŸŒŸ Introduction

DataFlow est un outil ETL (Extract-Transform-Load) modulaire dÃ©veloppÃ© en Python. ConÃ§u pour Ãªtre flexible et extensible, il permet de collecter, transformer et charger des donnÃ©es depuis et vers diverses sources.

## âœ¨ FonctionnalitÃ©s

### Extraction
- Lecture depuis des fichiers plats (CSV, TXT)
- Import depuis des fichiers structurÃ©s (JSON, XML, HTML)
- Connexion Ã  des bases de donnÃ©es relationnelles
- RÃ©cupÃ©ration de donnÃ©es via des API REST
- Support pour plusieurs sources simultanÃ©es

### Transformation
- Filtrage des donnÃ©es
- Nettoyage des valeurs manquantes ou aberrantes
- Calcul de nouvelles valeurs
- Normalisation des formats (dates, catÃ©gories, etc.)
- AgrÃ©gation et jointure de donnÃ©es

### Chargement
- Export vers des bases de donnÃ©es relationnelles
- GÃ©nÃ©ration de fichiers (CSV, JSON, XML)
- Support pour plusieurs destinations simultanÃ©es

## ğŸš€ Installation

1. Cloner le dÃ©pÃ´t :
   ```bash
   git clone [URL_DU_DEPOT]
   cd DataFlow
   ```

2. CrÃ©er un environnement virtuel :
   ```bash
   python -m venv venv
   source venv/bin/activate  # Sur Windows : venv\Scripts\activate
   ```

3. Installer les dÃ©pendances :
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ› ï¸ Utilisation

### Configuration
CrÃ©ez un fichier YAML de configuration pour dÃ©finir votre pipeline ETL :

```yaml
pipeline:
  - extract:
      type: csv
      source: data/input/data.csv
  - transform:
      - filter:
          condition: "age > 18"
      - normalize:
          field: date
          format: "%Y-%m-%d"
  - load:
      type: postgresql
      table: users
      connection: postgresql://user:password@localhost/dbname
```

### ExÃ©cution

```bash
python -m dataflow run pipeline.yaml
```

## ğŸ—ï¸ Architecture

L'application suit une architecture modulaire avec les composants principaux :

1. **Extract** : GÃ¨re la lecture des donnÃ©es depuis diffÃ©rentes sources
2. **Transform** : Applique les transformations aux donnÃ©es
3. **Load** : GÃ¨re l'Ã©criture des donnÃ©es transformÃ©es
4. **Core** : Contient les fonctionnalitÃ©s de base et les modÃ¨les de donnÃ©es

## ğŸ“ Structure du projet

```
DataFlow/
â”œâ”€â”€ data/                   # DonnÃ©es d'exemple
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract/           # Modules d'extraction
â”‚   â”‚   â”œâ”€â”€ csv.py
â”‚   â”‚   â”œâ”€â”€ api.py
â”‚   â”‚   â””â”€â”€ scrapping.py
â”‚   â”œâ”€â”€ transform/          # Modules de transformation
â”‚   â”‚   â””â”€â”€ processor.py
â”‚   â”œâ”€â”€ load/               # Modules de chargement
â”‚   â”‚   â”œâ”€â”€ csv.py
â”‚   â”‚   â””â”€â”€ sql.py
â”‚   â””â”€â”€ core/               # FonctionnalitÃ©s de base
â”œâ”€â”€ tests/                  # Tests unitaires
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ requirements.txt        # DÃ©pendances
â””â”€â”€ README.md
```

## ğŸ“š Documentation technique

Pour plus de dÃ©tails sur l'API et les modules, consultez la [documentation technique](./docs/).

## ğŸ‘¥ Contribution

Les contributions sont les bienvenues ! Voici comment contribuer :

1. Forkez le projet
2. CrÃ©ez une branche pour votre fonctionnalitÃ© (`git checkout -b feature/AmazingFeature`)
3. Committez vos modifications (`git commit -m 'Add some AmazingFeature'`)
4. Poussez vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

DÃ©veloppÃ© avec â¤ï¸ par [Votre Ã‰quipe]
